{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fe840-3ceb-4960-bce3-2903b67f5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern, hog\n",
    "from scipy.stats import moment\n",
    "import mahotas\n",
    "from skimage import measure\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100c98c-2d83-4e82-918f-64aa2d1a75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize an image\n",
    "def normalize_image(image):\n",
    "    return cv2.normalize(image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90b5fd-b709-4c64-a485-0cf9b3ae19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize an image\n",
    "def resize_image(image, size):\n",
    "    return cv2.resize(image, size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecdf524-53d6-4fe0-9a29-cee56080184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract color moments\n",
    "def extract_color_moments(image):\n",
    "    moments_r = moment(image[:, :, 0].ravel(), moment=[1, 2, 3])\n",
    "    moments_g = moment(image[:, :, 1].ravel(), moment=[1, 2, 3])\n",
    "    moments_b = moment(image[:, :, 2].ravel(), moment=[1, 2, 3])\n",
    "    return moments_r, moments_g, moments_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591284b3-1c9f-4929-9e67-3b2dc3f80f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Haralick texture features\n",
    "def extract_haralick_features(gray_image):\n",
    "    glcm = graycomatrix(gray_image, [1], [0], 256, symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
    "    entropy = graycoprops(glcm, 'ASM')[0, 0]\n",
    "    return contrast, correlation, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d481e7-7dfd-4d93-a441-6ebd8ae0fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract LBP features\n",
    "def extract_lbp_features(gray_image):\n",
    "    lbp = local_binary_pattern(gray_image, 8, 1, method='uniform')\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5290ff5-fbea-4eff-afb7-e5fe1e6332a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Zernike moments\n",
    "def extract_zernike_moments(gray_image):\n",
    "    return mahotas.features.zernike_moments(gray_image, radius=10, degree=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434548dd-79f3-459f-9adb-6873db8956a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Hu moments\n",
    "def extract_hu_moments(gray_image):\n",
    "    return cv2.HuMoments(cv2.moments(gray_image)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd032f9-a4ed-4555-91d8-96c3478c6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply Gabor filters and extract features\n",
    "def apply_gabor_filters(gray_image):\n",
    "    gabor_features = []\n",
    "    for theta in range(4):\n",
    "        theta = theta / 4. * np.pi\n",
    "        kernel = cv2.getGaborKernel((21, 21), 5.0, theta, 10.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "        fimg = cv2.filter2D(gray_image, cv2.CV_8UC3, kernel)\n",
    "        gabor_features.append(fimg.mean())\n",
    "        gabor_features.append(fimg.var())\n",
    "    return gabor_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e46e3-d1b1-4d7f-9ecc-2f9f84f5f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract contour-based features\n",
    "def extract_contour_features(binary_image, gray_image):\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    largest_contour = max(contours, key=cv2.contourArea) if contours else None\n",
    "    if largest_contour is None:\n",
    "        return None\n",
    "    area = cv2.contourArea(largest_contour)\n",
    "    perimeter = cv2.arcLength(largest_contour, True)\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    aspect_ratio = float(w) / h\n",
    "    contour_image = np.zeros_like(gray_image)\n",
    "    cv2.drawContours(contour_image, [largest_contour], -1, (255), thickness=cv2.FILLED)\n",
    "    eccentricity = measure.regionprops(measure.label(contour_image))[0].eccentricity\n",
    "    hull = cv2.convexHull(largest_contour)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    hull_perimeter = cv2.arcLength(hull, True)\n",
    "    solidity = area / hull_area\n",
    "    return area, perimeter, aspect_ratio, eccentricity, hull_area, hull_perimeter, solidity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605b4b2-7e60-4c3e-b805-7f0370803d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process images and extract features\n",
    "def process_images(dataset_dir, output_dir, input_size):\n",
    "    image_path = os.path.join(root, filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Normalize and resize the image\n",
    "    normalized_image = normalize_image(image)\n",
    "    resized_image = resize_image(normalized_image, input_size)\n",
    "    \n",
    "    # Save augmented images\n",
    "    save_augmented_images(resized_image, filename, subfolder_output_dir)\n",
    "\n",
    "    # Convert the resized image to grayscale\n",
    "    gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Extract features\n",
    "    moments_r, moments_g, moments_b = extract_color_moments(image)\n",
    "    contrast, correlation, entropy = extract_haralick_features(gray_image)\n",
    "    lbp_features = extract_lbp_features(gray_image)\n",
    "    zernike_moments = extract_zernike_moments(gray_image)\n",
    "    hu_moments = extract_hu_moments(gray_image)\n",
    "    gabor_features = apply_gabor_filters(gray_image)\n",
    "\n",
    "    # Extract HSV histograms\n",
    "    hsv_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2HSV)\n",
    "    hist_h = cv2.calcHist([hsv_image], [0], None, [256], [0, 256]).flatten()\n",
    "    hist_s = cv2.calcHist([hsv_image], [1], None, [256], [0, 256]).flatten()\n",
    "    hist_v = cv2.calcHist([hsv_image], [2], None, [256], [0, 256]).flatten()\n",
    "\n",
    "    # Extract GLCM properties\n",
    "    glcm = graycomatrix(gray_image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256, symmetric=True, normed=True)\n",
    "    glcm_props = [graycoprops(glcm, prop).ravel()[0] for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']]\n",
    "\n",
    "    # Extract contour-based features\n",
    "    binary_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    contour_features = extract_contour_features(binary_image, gray_image)\n",
    "    \n",
    "    if contour_features is not None:\n",
    "        area, perimeter, aspect_ratio, eccentricity, hull_area, hull_perimeter, solidity = contour_features\n",
    "        \n",
    "      features = np.concatenate((\n",
    "            ['Contrast': contrast, 'Dissimilarity': glcm_props[1], 'Homogeneity': glcm_props[2], 'Energy': glcm_props[3],\n",
    "            'Correlation': glcm_props[4],'Area': area, 'Perimeter': perimeter,'Aspect_Ratio': aspect_ratio, \n",
    "            'Eccentricity': eccentricity, 'Hull_Area': hull_area, 'Hull_Perimeter': hull_perimeter, 'Solidity': solidity,]\n",
    "            'Hue_Histogram': hist_h.tolist(), 'Saturation_Histogram': hist_s.tolist(), 'Value_Histogram': hist_v.tolist(),\n",
    "            'Moments_R': moments_r.tolist(), 'Moments_G': moments_g.tolist(), 'Moments_B': moments_b.tolist(),\n",
    "            'LBP_Features': lbp_features.tolist(),'Zernike_Moments': zernike_moments.tolist(), 'Hu_Moments': hu_moments.tolist(),\n",
    "            'Gabor_Features': gabor_features\n",
    "      ))\n",
    "       \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a57c1-ce23-4618-8c40-7b39a24b84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler\n",
    "scaler_path = r\"C:\\Users\\User\\Desktop\\exprement\\logistic_regression_scaler.pkl\"\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# Standardize the features\n",
    "features_scaled = scaler.transform(features.reshape(1, -1))\n",
    "\n",
    "# Load the model\n",
    "model_path =  r'C:\\Users\\User\\Desktop\\exprement\\logistic_regression_model.pkl'\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(features_scaled)\n",
    "\n",
    "print(f\"Predicted class: {prediction[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceaef84-fc27-4272-8582-ed34752be571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be907795-a1b2-482f-bc58-7b303843daa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f6e6d-e037-4566-bf06-1a9e11fa3250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a919a-4681-4447-a7a5-b4aa6767c20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72fd42d-996c-4b67-9fd0-ea15c318d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from skimage import feature, measure\n",
    "import pandas as pd\n",
    "\n",
    "# Load the test image\n",
    "image_path = r\"C:\\Users\\User\\Desktop\\sub\\final2_image.png\"\n",
    "image = cv2.imread(image_path)\n",
    "# Define the desired input size for the CNN\n",
    "input_size = (224, 224)\n",
    "# Normalization\n",
    "normalized_image = cv2.normalize(image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "# Resize the image to match the input size of the CNN\n",
    "resized_image = cv2.resize(normalized_image, input_size)\n",
    "\n",
    "# Convert the resized image to grayscale\n",
    "gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Calculate the color histograms for each channel\n",
    "hsv_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2HSV)\n",
    "hist_h = cv2.calcHist([hsv_image], [0], None, [256], [0, 256])\n",
    "hist_s = cv2.calcHist([hsv_image], [1], None, [256], [0, 256])\n",
    "hist_v = cv2.calcHist([hsv_image], [2], None, [256], [0, 256])\n",
    "\n",
    "# Compute the GLCM\n",
    "glcm = feature.graycomatrix(gray_image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256, symmetric=True, normed=True)\n",
    "\n",
    "# Calculate texture features\n",
    "contrast = feature.graycoprops(glcm, 'contrast').ravel()[0]\n",
    "dissimilarity = feature.graycoprops(glcm, 'dissimilarity').ravel()[0]\n",
    "homogeneity = feature.graycoprops(glcm, 'homogeneity').ravel()[0]\n",
    "energy = feature.graycoprops(glcm, 'energy').ravel()[0]\n",
    "\n",
    "# Threshold the image to obtain a binary image\n",
    "_, binary_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "# Find contours in the binary image\n",
    "contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Find the largest contour by area\n",
    "largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "# Calculate shape features\n",
    "area = cv2.contourArea(largest_contour)\n",
    "perimeter = cv2.arcLength(largest_contour, True)\n",
    "\n",
    "# Convert the contour to a 2D image\n",
    "contour_image = np.zeros_like(gray_image)\n",
    "cv2.drawContours(contour_image, [largest_contour], 0, (255, 255, 255), thickness=cv2.FILLED)\n",
    "\n",
    "# Calculate eccentricity on the 2D image\n",
    "eccentricity = measure.regionprops(measure.label(contour_image))[0].eccentricity\n",
    "\n",
    "# Calculate the convex hull of the largest contour\n",
    "hull = cv2.convexHull(largest_contour)\n",
    "\n",
    "# Calculate the area of the convex hull\n",
    "hull_area = cv2.contourArea(hull)\n",
    "\n",
    "# Calculate the perimeter of the convex hull\n",
    "hull_perimeter = cv2.arcLength(hull, True)\n",
    "\n",
    "# Calculate the solidity of the object (ratio of contour area to convex hull area)\n",
    "solidity = cv2.contourArea(largest_contour) / hull_area\n",
    "\n",
    "# Combine all features into a single feature vector\n",
    "features = np.concatenate((hist_h.flatten(), hist_s.flatten(), hist_v.flatten(),\n",
    "                           [contrast, dissimilarity, homogeneity, energy,\n",
    "                            area, perimeter, eccentricity, hull_area,\n",
    "                            hull_perimeter, solidity]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce1b88-aea9-416c-a2fd-8aa21145b5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276d37e-5809-4034-ab1e-8745db4e18cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab376f-7a9f-46da-bb47-dea92ed90010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb20c9-984f-4c39-a03d-13cd89d04be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe5f3e0-56e9-4f33-9b00-3cd978acff35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871eaa00-3691-4c22-9bab-e50b45e4eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the scaler and model files\n",
    "models = {\n",
    "    \"RandomForest\": {\n",
    "        \"scaler_path\": r\"C:\\Users\\User\\Desktop\\exprement\\random_forest_scaler.pkl\",\n",
    "        \"model_path\": r'C:\\Users\\User\\Desktop\\exprement\\random_forest_model.pkl'\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"scaler_path\": r\"C:\\Users\\User\\Desktop\\exprement\\logistic_regression_scaler.pkl\",\n",
    "        \"model_path\": r'C:\\Users\\User\\Desktop\\exprement\\logistic_regression_model.pkl'\n",
    "    },\n",
    "\n",
    "    \"SVM\": {\n",
    "        \"scaler_path\": r\"C:\\Users\\User\\Desktop\\exprement\\svm_scaler.pkl\",\n",
    "        \"model_path\": r'C:\\Users\\User\\Desktop\\exprement\\svm_model.pkl'\n",
    "    },\n",
    "    \"GBM\": {\n",
    "        \"scaler_path\": r\"C:\\Users\\User\\Desktop\\exprement\\gbm_scaler.pkl\",\n",
    "        \"model_path\": r'C:\\Users\\User\\Desktop\\exprement\\gbm_model.pkl'\n",
    "    },\n",
    "\n",
    "    \"KNN\": {\n",
    "        \"scaler_path\": r\"C:\\Users\\User\\Desktop\\exprement\\knn_scaler.pkl\",\n",
    "        \"model_path\": r'C:\\Users\\User\\Desktop\\exprement\\knn_model.pkl'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Iterate over the models and make predictions\n",
    "for model_name, paths in models.items():\n",
    "    scaler = joblib.load(paths[\"scaler_path\"])\n",
    "    features_scaled = scaler.transform(features.reshape(1, -1))\n",
    "    model = joblib.load(paths[\"model_path\"])\n",
    "    prediction = model.predict(features_scaled)\n",
    "    print(f\"Model: {model_name}, Predicted class: {prediction[0]}\")\n",
    "\n",
    "# # Generate synthetic ground truth labels (assuming 5 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab394a-4a62-480a-b851-a531c7d1d743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c5fb7-d547-4e62-9566-d2569410b061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd8d63-18d5-4b1e-99df-cf5316f0c748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84883a08-abf7-4ad0-919d-33c502e45e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac310c-0824-4979-b165-c0ac9b8a5703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab19b16-8190-4f02-92e3-3c495fd51580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cca11e-e34c-4e2b-a41e-aa2e0795f172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51ef47-85b7-4044-9a33-6c6d6c7ddb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0cd8e-fae3-4ffa-a1bb-f51df94e2368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acb45a-c3af-4720-bff1-15d01b04ee05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
