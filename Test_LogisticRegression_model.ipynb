{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8326e6d9-5bff-4504-bb34-5fb7a9d8497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import mahotas.features\n",
    "import skimage.measure as measure\n",
    "from skimage import feature, measure\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from skimage import io, color, feature\n",
    "from scipy.stats import moment\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import ast\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from skimage import feature, measure\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb003759-b78f-44c4-bff7-a76ee20435d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7563189f-f49f-45b8-ab74-24711ecc0714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 27085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 27085 features, but StandardScaler is expecting 795 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Standardize the features\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(features\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Make a prediction\u001b[39;00m\n\u001b[0;32m    124\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(features_scaled)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:992\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    989\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    991\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m--> 992\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    993\u001b[0m     X,\n\u001b[0;32m    994\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    995\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    996\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    997\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    998\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    999\u001b[0m )\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 27085 features, but StandardScaler is expecting 795 features as input."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from scipy.stats import moment\n",
    "import mahotas\n",
    "import joblib\n",
    "from skimage import measure\n",
    "\n",
    "# Load the test image\n",
    "image_path = r\"C:\\Users\\User\\Downloads\\IMG_20240501_184017.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was loaded successfully\n",
    "if image is None:\n",
    "    print(f\"Error: Unable to load image at path {image_path}\")\n",
    "else:\n",
    "    input_size = (224, 224)\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_image = cv2.normalize(image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "    # Resize the image to match the input size of the CNN\n",
    "    resized_image = cv2.resize(normalized_image, input_size)\n",
    "\n",
    "    # Convert the resized image to grayscale\n",
    "    gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate color moments for each channel\n",
    "    moments_r = moment(image[:, :, 0].ravel(), moment=[1, 2, 3])\n",
    "    moments_g = moment(image[:, :, 1].ravel(), moment=[1, 2, 3])\n",
    "    moments_b = moment(image[:, :, 2].ravel(), moment=[1, 2, 3])\n",
    "\n",
    "    # Calculate Haralick texture features\n",
    "    glcm = graycomatrix(gray_image, [1], [0], 256, symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
    "    entropy = graycoprops(glcm, 'ASM')[0, 0]\n",
    "\n",
    "    # Calculate Histogram of Oriented Gradients (HOG)\n",
    "    hog_features = feature.hog(gray_image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys', feature_vector=True)\n",
    "\n",
    "    # Calculate Local Binary Patterns (LBP)\n",
    "    lbp = local_binary_pattern(gray_image, 8, 1, method='uniform')\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))\n",
    "\n",
    "    # Calculate Zernike Moments\n",
    "    zernike_moments = mahotas.features.zernike_moments(gray_image, radius=10, degree=8)\n",
    "\n",
    "    # Calculate Hu Moments\n",
    "    moments_hu = cv2.HuMoments(cv2.moments(gray_image)).flatten()\n",
    "\n",
    "    # Apply Gabor filters\n",
    "    gabor_features = []\n",
    "    kernels = []\n",
    "    for theta in range(4):\n",
    "        theta = theta / 4. * np.pi\n",
    "        kernel = cv2.getGaborKernel((21, 21), 5.0, theta, 10.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "        kernels.append(kernel)\n",
    "        fimg = cv2.filter2D(gray_image, cv2.CV_8UC3, kernel)\n",
    "        gabor_features.append(fimg.mean())\n",
    "        gabor_features.append(fimg.var())\n",
    "\n",
    "    # Feature extraction\n",
    "    hsv_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2HSV)\n",
    "    hist_h = cv2.calcHist([hsv_image], [0], None, [256], [0, 256])\n",
    "    hist_s = cv2.calcHist([hsv_image], [1], None, [256], [0, 256])\n",
    "    hist_v = cv2.calcHist([hsv_image], [2], None, [256], [0, 256])\n",
    "\n",
    "    glcm = feature.graycomatrix(gray_image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256, symmetric=True, normed=True)\n",
    "    contrast = feature.graycoprops(glcm, 'contrast').ravel()[0]\n",
    "    dissimilarity = feature.graycoprops(glcm, 'dissimilarity').ravel()[0]\n",
    "    homogeneity = feature.graycoprops(glcm, 'homogeneity').ravel()[0]\n",
    "    energy = feature.graycoprops(glcm, 'energy').ravel()[0]\n",
    "    correlation = feature.graycoprops(glcm, 'correlation').ravel()[0]\n",
    "\n",
    "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    largest_contour = max(contours, key=cv2.contourArea) if contours else None\n",
    "\n",
    "    if largest_contour is not None:\n",
    "        area = cv2.contourArea(largest_contour)\n",
    "        perimeter = cv2.arcLength(largest_contour, True)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        aspect_ratio = float(w) / h\n",
    "        contour_image = np.zeros_like(gray_image)\n",
    "        cv2.drawContours(contour_image, [largest_contour], -1, (255), thickness=cv2.FILLED)\n",
    "        eccentricity = measure.regionprops(measure.label(contour_image))[0].eccentricity\n",
    "        hull = cv2.convexHull(largest_contour)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        hull_perimeter = cv2.arcLength(hull, True)\n",
    "        solidity = area / hull_area\n",
    "\n",
    "    # Combine all features into a single feature vector\n",
    "    features = np.concatenate((\n",
    "                             hist_h.flatten().tolist(),\n",
    "                             hist_s.flatten().tolist(),\n",
    "                             hist_v.flatten().tolist(),\n",
    "                             [contrast], [dissimilarity],\n",
    "                             [homogeneity],  [energy],  [correlation],\n",
    "                             [area],  [perimeter], [aspect_ratio],\n",
    "                             [eccentricity],  [hull_area],\n",
    "                             [hull_perimeter],  [solidity],\n",
    "                             [moments_r[0]], [moments_r[1]],  [moments_r[2]],\n",
    "                             [moments_g[0]], [moments_g[1]], [moments_g[2]],\n",
    "                             [moments_b[0]],  [moments_b[1]], [moments_b[2]],\n",
    "                             [contrast],  [correlation],  [entropy],\n",
    "                             hog_features.tolist(),  hist.tolist(),\n",
    "                             zernike_moments.tolist(),  moments_hu.tolist(),\n",
    "                             gabor_features))\n",
    "\n",
    "    print(f\"Number of features: {features.size}\")  # Print the number of features\n",
    "\n",
    "    # Define paths to the scaler and model files\n",
    "    scaler_path = r\"C:\\Users\\User\\Desktop\\project\\model_output\\logistic_regression_scaler.pkl\"\n",
    "    model_path =  r\"C:\\Users\\User\\Desktop\\project\\model_output\\logistic_regression_model.pkl\"\n",
    "\n",
    "    # Load the scaler and model\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # Standardize the features\n",
    "    features_scaled = scaler.transform(features.reshape(1, -1))\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(features_scaled)\n",
    "\n",
    "    print(f\"Predicted class: {prediction[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff2b49-a82d-454b-9ca6-9d06af739685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18952266-0e68-4cb3-8c5b-dfe1f592a977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7c924-18d5-4ddc-9847-e18392459469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd04f2-b1c4-40a8-8455-1d05e8d66e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca2e70-1ff9-4051-b956-cf7b3795b453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1572f-e5bd-4f4a-a0d5-a6c047239894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
