{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989f4e87-98e4-42f9-a96e-14c8a13310da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Classifier Predicted Class: Jatropha\n",
      "Logistic Regression Predicted Class: Platanus orientalis\n",
      "KNeighbors Classifier Predicted Class: Ocimum basilicum\n",
      "Gradient Boostingv Classifier Predicted Class: Ocimum basilicum\n",
      "Decision Tree Classifier Predicted Class: Citrus aurantiifolia\n",
      "SVM classifier Predicted Class: Ocimum basilicum\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib  # Used for loading the trained model\n",
    "import json \n",
    "from scipy.stats import moment\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.feature import graycomatrix as graycomatrix, graycoprops as graycoprops\n",
    "from skimage import measure\n",
    "import mahotas\n",
    "import ast\n",
    "\n",
    "# Function to normalize an image\n",
    "def normalize_image(image):\n",
    "    return cv2.normalize(image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "# Function to resize an image\n",
    "def resize_image(image, size):\n",
    "    return cv2.resize(image, size)\n",
    "\n",
    "# Function to extract color moments\n",
    "def extract_color_moments(image):\n",
    "    moments_r = moment(image[:, :, 0].ravel(), moment=[1, 2, 3])\n",
    "    moments_g = moment(image[:, :, 1].ravel(), moment=[1, 2, 3])\n",
    "    moments_b = moment(image[:, :, 2].ravel(), moment=[1, 2, 3])\n",
    "    return moments_r, moments_g, moments_b\n",
    "\n",
    "# Function to extract Haralick texture features\n",
    "def extract_haralick_features(gray_image):\n",
    "    glcm = graycomatrix(gray_image, [1], [0], 256, symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
    "    entropy = graycoprops(glcm, 'ASM')[0, 0]\n",
    "    return contrast, correlation, entropy\n",
    "\n",
    "# Function to extract LBP features\n",
    "def extract_lbp_features(gray_image):\n",
    "    lbp = local_binary_pattern(gray_image, 8, 1, method='uniform')\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))\n",
    "    return hist\n",
    "\n",
    "# Function to extract Zernike moments\n",
    "def extract_zernike_moments(gray_image):\n",
    "    return mahotas.features.zernike_moments(gray_image, radius=10, degree=8)\n",
    "\n",
    "# Function to extract Hu moments\n",
    "def extract_hu_moments(gray_image):\n",
    "    return cv2.HuMoments(cv2.moments(gray_image)).flatten()\n",
    "\n",
    "# Function to apply Gabor filters and extract features\n",
    "def apply_gabor_filters(gray_image):\n",
    "    gabor_features = []\n",
    "    for theta in range(4):\n",
    "        theta = theta / 4. * np.pi\n",
    "        kernel = cv2.getGaborKernel((21, 21), 5.0, theta, 10.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "        fimg = cv2.filter2D(gray_image, cv2.CV_8UC3, kernel)\n",
    "        gabor_features.append(fimg.mean())\n",
    "        gabor_features.append(fimg.var())\n",
    "    return gabor_features\n",
    "\n",
    "# Function to extract contour-based features\n",
    "def extract_contour_features(binary_image, gray_image):\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    largest_contour = max(contours, key=cv2.contourArea) if contours else None\n",
    "    if largest_contour is None:\n",
    "        return None\n",
    "    area = cv2.contourArea(largest_contour)\n",
    "    perimeter = cv2.arcLength(largest_contour, True)\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    aspect_ratio = float(w) / h\n",
    "    contour_image = np.zeros_like(gray_image)\n",
    "    cv2.drawContours(contour_image, [largest_contour], -1, (255), thickness=cv2.FILLED)\n",
    "    eccentricity = measure.regionprops(measure.label(contour_image))[0].eccentricity\n",
    "    hull = cv2.convexHull(largest_contour)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    hull_perimeter = cv2.arcLength(hull, True)\n",
    "    solidity = area / hull_area\n",
    "    return area, perimeter, aspect_ratio, eccentricity, hull_area, hull_perimeter, solidity\n",
    "\n",
    "# Function to process a single image and extract features\n",
    "def process_single_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Normalize and resize the image\n",
    "    normalized_image = normalize_image(image)\n",
    "    resized_image = resize_image(normalized_image, input_size)\n",
    "\n",
    "    # Convert the resized image to grayscale\n",
    "    gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Extract features\n",
    "    moments_r, moments_g, moments_b = extract_color_moments(image)\n",
    "    contrast, correlation, entropy = extract_haralick_features(gray_image)\n",
    "    lbp_features = extract_lbp_features(gray_image)\n",
    "    zernike_moments = extract_zernike_moments(gray_image)\n",
    "    hu_moments = extract_hu_moments(gray_image)\n",
    "    gabor_features = apply_gabor_filters(gray_image)\n",
    "\n",
    "    # Extract HSV histograms\n",
    "    hsv_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2HSV)\n",
    "    hist_h = cv2.calcHist([hsv_image], [0], None, [256], [0, 256]).flatten()\n",
    "    hist_s = cv2.calcHist([hsv_image], [1], None, [256], [0, 256]).flatten()\n",
    "    hist_v = cv2.calcHist([hsv_image], [2], None, [256], [0, 256]).flatten()\n",
    "\n",
    "    # Extract GLCM properties\n",
    "    glcm = graycomatrix(gray_image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256, symmetric=True, normed=True)\n",
    "    glcm_props = [graycoprops(glcm, prop).ravel()[0] for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']]\n",
    "\n",
    "    # Extract contour-based features\n",
    "    binary_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    contour_features = extract_contour_features(binary_image, gray_image)\n",
    "    \n",
    "    if contour_features is not None:\n",
    "        area, perimeter, aspect_ratio, eccentricity, hull_area, hull_perimeter, solidity = contour_features\n",
    "        features = {\n",
    "                    'Contrast': contrast, 'Dissimilarity': glcm_props[1], 'Homogeneity': glcm_props[2], 'Energy': glcm_props[3],\n",
    "                    'Correlation': glcm_props[4],'Area': area, 'Perimeter': perimeter,'Aspect_Ratio': aspect_ratio, \n",
    "                    'Eccentricity': eccentricity, 'Hull_Area': hull_area, 'Hull_Perimeter': hull_perimeter, 'Solidity': solidity,\n",
    "                    'Hue_Histogram': hist_h.tolist(), 'Saturation_Histogram': hist_s.tolist(), 'Value_Histogram': hist_v.tolist(),\n",
    "                    'Moments_R': moments_r.tolist(), 'Moments_G': moments_g.tolist(), 'Moments_B': moments_b.tolist(),\n",
    "                    'LBP_Features': lbp_features.tolist(),'Zernike_Moments': zernike_moments.tolist(), 'Hu_Moments': hu_moments.tolist(),\n",
    "                    'Gabor_Features': gabor_features\n",
    "                }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "        return None\n",
    "# Function to process list columns to match training data format\n",
    "def process_list_column(features):\n",
    "    processed_features = {}\n",
    "    for col, values in features.items():\n",
    "        if isinstance(values, list):\n",
    "            for i, value in enumerate(values):\n",
    "                processed_features[f\"{col}_{i+1}\"] = value\n",
    "        else:\n",
    "            processed_features[col] = values\n",
    "    return processed_features\n",
    "\n",
    "# Load the trained models\n",
    "models = {\n",
    "    'RandomForest Classifier': joblib.load(r\"C:\\Users\\havar\\Downloads\\Project\\ML_model_output\\RandomForest_Classifier_model.pkl\"),\n",
    "    'Logistic Regression': joblib.load(r\"C:\\Users\\havar\\Downloads\\Project\\ML_model_output\\logistic_regression_model.pkl\"),\n",
    "    'KNeighbors Classifier': joblib.load(r\"C:\\Users\\havar\\Downloads\\Project\\ML_model_output\\KNeighbors_Classifier_model.pkl\"),\n",
    "    'Gradient Boostingv Classifier': joblib.load(r\"C:\\Users\\havar\\Downloads\\Project\\ML_model_output\\GradientBoosting_Classifier_model.pkl\"),\n",
    "    'Decision Tree Classifier': joblib.load(r\"C:\\Users\\havar\\Downloads\\Project\\ML_model_output\\DecisionTreeClassifier_model.pkl\"),\n",
    "    'SVM classifier': joblib.load(r\"C:\\Users\\havar\\Downloads\\Project\\ML_model_output\\SVM_classifier_model.pkl\")\n",
    "}\n",
    "\n",
    "# Path to the single image to test\n",
    "image_path = r\"C:\\Users\\havar\\Downloads\\Project\\Leaf Images Database\\Terminalia arjuna\\0002_0187.JPG\"\n",
    "input_size = (224, 224)  # Set the input size as used during training\n",
    "\n",
    "# Process the single image\n",
    "features = process_single_image(image_path, input_size)\n",
    "if features:\n",
    "    # Process the features to match the training data format\n",
    "    processed_features = process_list_column(features)\n",
    "    \n",
    "    # Convert to DataFrame and match the order of columns used during training\n",
    "    feature_df = pd.DataFrame([processed_features])\n",
    "    training_columns = pd.read_csv(r\"C:\\Users\\havar\\Downloads\\Project\\handcrafted_features\\extracted.csv\").columns\n",
    "    training_columns = training_columns.drop(['Class', 'Filename'])  \n",
    "    feature_df = feature_df.reindex(columns=training_columns, fill_value=0)\n",
    "    # Remove feature names from the DataFrame\n",
    "    feature_df.columns = range(feature_df.shape[1])\n",
    "    \n",
    "            # Make predictions\n",
    "    predictions = {}\n",
    "    for model_name, model in models.items():\n",
    "        prediction = model.predict(feature_df)\n",
    "        predictions[model_name] = {'prediction': prediction[0]}\n",
    "        \n",
    "    # Print predictions\n",
    "    for model_name, result in predictions.items():\n",
    "        prediction_str = str(result[\"prediction\"]) if isinstance(result[\"prediction\"], np.ndarray) else str(result[\"prediction\"])\n",
    "        print(f'{model_name} Predicted Class: {prediction_str}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d907564e-8bb8-47d4-9611-e9d7f03b8235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mahotas\n",
      "  Downloading mahotas-1.4.15-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\havar\\anaconda3\\lib\\site-packages (from mahotas) (1.26.4)\n",
      "Downloading mahotas-1.4.15-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.7 MB 660.6 kB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/1.7 MB 1.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/1.7 MB 1.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/1.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.6/1.7 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.7/1.7 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.9/1.7 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.1/1.7 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.2/1.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.5/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: mahotas\n",
      "Successfully installed mahotas-1.4.15\n"
     ]
    }
   ],
   "source": [
    "!pip install mahotas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d62cc-c00a-4f1d-9f1b-a145bce17ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f844e-9add-4fba-b9f9-be871a0fc7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab59c5c-e3c3-4066-a71b-8595a988806a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
